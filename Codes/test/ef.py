# -*- coding: utf-8 -*-
"""EF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pJcx1p-Ym7DbqgK5EfwrzDGLMmZQiyc7
"""

import tensorflow as tf
tf.compat.v1.disable_v2_behavior()
from tensorflow.keras.models import *
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import *
from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler
from tensorflow.keras import backend as kb

import numpy as np
import pandas as pd

tf.get_logger().setLevel('ERROR')
# !pip install tensorflow-privacy
import tensorflow_privacy
from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy
import tensorflow.keras.backend as kb
import h5py

### parameters

n = 10 
m = 20


batch_size = 256
train_samples = batch_size*1200
test_samples = batch_size*100

learning_rate = 1e-3
num_epochs = 130
rho = 1

## DATA
def gen_data(bs, n, m):
    data = np.random.uniform(0,1,size=(bs, n, m, 1))
    return data

train_data = gen_data(train_samples, n, m)
test_data = gen_data(test_samples, n, m)
test_data.shape
train_data.shape

"""### MODEL"""

def EF_NET(pretrained_weights = None, input_size = (10,20,1)):
    inputs = Input(input_size)
    conv1 = Conv2D(8, 3, activation = 'tanh', kernel_initializer = 'he_normal')(inputs)
    conv2 = Conv2D(16, 3, activation = 'tanh',  kernel_initializer = 'he_normal')(conv1)
    conv3 = Conv2D(32, 3, activation = 'tanh',  kernel_initializer = 'he_normal')(conv2)
    conv4 = Conv2D(64, 3, activation = 'tanh',  kernel_initializer = 'he_normal')(conv3)
    up1 = Conv2DTranspose(32, 3, activation = 'tanh', kernel_initializer = 'he_normal')(conv4)
    up2 = Conv2DTranspose(16, 3, activation = 'tanh', kernel_initializer = 'he_normal')(up1)
    up3 = Conv2DTranspose(8, 3, activation = 'tanh', kernel_initializer = 'he_normal')(up2)
    up4 = Conv2DTranspose(1, 3, activation = 'tanh', kernel_initializer = 'he_normal')(up3)
    conv11 = Conv2D(1, 3, activation = 'tanh', kernel_initializer = 'he_normal')(up4)
    conv12 = Conv2D(16, 3, activation = 'tanh',  kernel_initializer = 'he_normal')(conv11)
    conv13 = Conv2D(32, 3, activation = 'tanh',  kernel_initializer = 'he_normal')(conv12)
    conv14 = Conv2D(64, 3, activation = 'tanh',  kernel_initializer = 'he_normal')(conv13)
    up11 = Conv2DTranspose(32, 3, activation = 'tanh', kernel_initializer = 'he_normal')(conv14)
    up12 = Conv2DTranspose(16, 3, activation = 'tanh', kernel_initializer = 'he_normal')(up11)
    up13 = Conv2DTranspose(8, 3, activation = 'tanh', kernel_initializer = 'he_normal')(up12)
    up14 = Conv2DTranspose(1, 3, activation = 'tanh', kernel_initializer = 'he_normal')(up13)
    conv21 = Conv2D(8, 3, activation = 'tanh', kernel_initializer = 'he_normal')(up14)
    conv22 = Conv2D(16, 3, activation = 'tanh',  kernel_initializer = 'he_normal')(conv21)
    conv23 = Conv2D(32, 3, activation = 'tanh',  kernel_initializer = 'he_normal')(conv22)
    conv24 = Conv2D(64, 3, activation = 'tanh',  kernel_initializer = 'he_normal')(conv23)
    up21 = Conv2DTranspose(32, 3, activation = 'tanh', kernel_initializer = 'he_normal')(conv24)
    up22 = Conv2DTranspose(16, 3, activation = 'tanh', kernel_initializer = 'he_normal')(up21)
    up23 = Conv2DTranspose(8, 3, activation = 'tanh', kernel_initializer = 'he_normal')(up22)
    up24 = Conv2DTranspose(1, 3, activation = 'tanh', kernel_initializer = 'he_normal')(up23)

    # up5 = Conv2DTranspose(64, 3, activation = 'tanh', kernel_initializer = 'he_normal')(up4)
    # up2 = Conv2D(32, 3, activation = 'tanh', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(up1))
    # up3 = Conv2D(16, 3, activation = 'tanh', kernel_initializer = 'he_normal')(UpSampling2D(size = (3,3))(up2))
    # up4 = Conv2D(8, 3, activation = 'tanh', kernel_initializer = 'he_normal')(UpSampling2D(size = (3,3))(up3))
    # up5 = Conv2D(1, 3, activation = 'tanh', kernel_initializer = 'he_normal')(UpSampling2D(size = (3,3))(up4))

    # conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)
    # pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
    # conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)
    # conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)
    # pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
#     conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)
#     conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)
#     pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)
#     conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)
#     conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)
#     drop4 = Dropout(0.5)(conv4)
#     pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)

#     conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)
#     conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)
    # drop5 = Dropout(0.5)(conv2)

#     up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))
#     merge6 = concatenate([drop4,up6], axis = 3)
#     conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)
#     conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)

#     up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))
#     merge7 = concatenate([conv3,up7], axis = 3)
#     conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)
#     conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)

    # up8 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))
#     merge8 = concatenate([conv2,up8], axis = 3)
#     conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)
#     conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)

#     up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(up8))
#     merge9 = concatenate([conv1,up9], axis = 3)
#     conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)
#     conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)
#     conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)
    # conv10 = Conv2D(1, 1, activation = 'linear')(up8)

    final = tf.keras.activations.softmax(99*up24, axis = 1)

    model = Model(inputs = inputs, outputs = final)

#     model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])
    
    model.summary()

#     if(pretrained_weights):
#         model.load_weights(pretrained_weights)

    return model

# # TEST BLOCK
# # A = np.random.rand(1000, 10,20)
# # B = np.random.rand(1000, 20,10)

# # A = np.array([1,1,1,1]).reshape((1,2,2))
# # B = np.array([1,2,1,1]).reshape((1,2,2))
# # x = kb.variable(value=A)
# # y = kb.variable(value=B)

# # z = kb.dot(x,y)[:,:,0,:]
# # out = z - tf.expand_dims(tf.linalg.diag_part(z), axis=-1)
# # print(out.shape)

# out = tf.reshape(tf.constant([1.0,2,3,4,5,6,7,2,4,8, 7, 6]), (2,2,3))
# print(kb.eval(out))
# # print(kb.permute_dimensions(out,(0,2,1)).shape)
# # out = tf.math.reduce_prod(out, axis=1)
# # out = tf.keras.activations.softmax(out, axis =1)
# x = tf.math.reduce_max(out, 1)
# print(kb.eval(x))
# #kb.eval(kb.sum(out))

#### TRAIN ####

def ef_loss(inp, allocation):
    inp = inp[:,:,:,0]
    allocation = allocation[:,:,:,0]
    
    sw_obt = inp * allocation

    val = tf.matmul(inp, kb.permute_dimensions(allocation,(0,2,1)))
    val_i = tf.linalg.diag_part(val)

    vali = tf.expand_dims(val_i, axis =-1)    
    val = kb.maximum(val - vali, 0)

    return kb.sum(val)  - kb.sum(sw_obt)

def ef1(inp, allocation, temperature = 99999, trainMode=True):
    inp = inp[:,:,:,0]
    allocation = tf.nn.softmax(allocation*temperature , axis=1)
    # alloc = tf.nn.softmax(allocation*temperature , axis=1)
    allocation = allocation[:,:,:,0]

    # print(kb.eval(allocation))
    # multiply = tf.multiply(inp[:,:,None,:] , allocation)
    
    maxvalueditemlist = []
    for i in range(allocation.shape[1]):
      multiply = inp[:,i,None,:] * allocation
      maxvalueditem =tf.math.reduce_max(multiply, axis = 2) 
      maxvalueditemlist.append(maxvalueditem)
    
    maxvalueditemlist  = tf.stack(maxvalueditemlist)


    val = tf.matmul(inp, kb.permute_dimensions(allocation,(0,2,1)))
    val_i = tf.linalg.diag_part(val)
    vali = tf.expand_dims(val_i, axis =-1)    

    val = kb.maximum(val - vali - kb.permute_dimensions(maxvalueditemlist,(1,0,2)), 0.0)
    if(trainMode):
      return kb.sum(val)
    else:
      return val


def calculateEnvySamplewise(inp,allocation):
  n_samples = allocation.shape[0]
  ef1loss = ef1(inp,allocation,99999, False)
  envypersamples = kb.sum(kb.sum(ef1loss,2),1)
  envysamples =  tf.math.count_nonzero(envypersamples)
  count =  kb.eval(n_samples - envysamples)
  print('___________________________________________')
  print(' Total Envy Free samples ', count)
  print(' Total Envy Free samples percent', kb.eval(tf.math.divide(count,n_samples)))
  return kb.eval(tf.math.divide(count,n_samples))

 

def sw(inp, allocation):
    inp = inp[:,:,:,0]
    allocation = allocation[:,:,:,0]
    sw = kb.sum(kb.max(inp, 2))
    sw_obt = inp * allocation
    return sw - sw_obt

def calculatesw(inp,allocation):
    inp = inp[:,:,:,0]
    allocation = allocation[:,:,:,0]
    sw_obt = inp * allocation
    sw_obt = kb.sum(kb.sum(sw_obt, axis=2),axis=1)
    sw_obt = tf.math.reduce_mean(sw_obt)
    return sw_obt


def train():
    model = EF_NET((n,m,1))
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
    
    model.compile(optimizer=optimizer, loss= ef_loss, metrics=[ef1,sw])
    es = tf.keras.callbacks.EarlyStopping(monitor='val_ef_loss', mode='min', min_delta=0.01, patience=200)
    model.fit(train_data, train_data,
                  epochs=num_epochs,
                  validation_data=(test_data, test_data),
                  batch_size=batch_size, callbacks=[es])
    return model

# # print(kb.eval(ef_loss(tf.random.uniform((10,3,4,1)), tf.random.uniform((10,3,4,1)))))

# valuation_profile = tf.reshape(tf.constant([22,41,202,5,29,31,30,5,31,41,30,22,130,230,202,500,245,265,210,203,265,230,210,130],dtype=tf.float64), (2,3,4,1))

# # print(valuation_profile.shape)
# # print(kb.eval(valuation_profile))
# allocation_profile = tf.reshape(tf.constant([1,0,0,0,
#                                              0,1,0,0,
#                                              0,0,1,1,
#                                              1,0,0,0,
#                                              0,1,1,0,
#                                              0,0,0,1],dtype=tf.float64), (2,3,4,1))
# # print(kb.eval(allocation_profile))
# # print(kb.eval(ef_loss(tf.random.uniform((10,3,4,1)), tf.random.uniform((10,3,4,1)))))

# # print(kb.eval(ef_loss(valuation_profile,allocation_profile )))

# print(kb.eval(calculatesw(valuation_profile,allocation_profile)))

# # print(kb.eval(ef1(valuation_profile,allocation_profile ,9999,False)))

# # kb.eval(calculateEnvySamplewise(valuation_profile,allocation_profile))

# # alloc = tf.nn.softmax(valuation_profile , axis=1)
# # print(kb.eval(alloc[:,:,:,0]))

trained_model = train()

# # kb.eval(trained_model(test_data[0:1])[0])
# alloc1 = kb.eval(trained_model(test_data[0:2]))
# # print(tf.constant(alloc1))
# # print('__________________________________________________-')
# # print(tf.constant(test_data[0:1],dtype=tf.float32))
# envyvalue = calculateEnvySamplewise(tf.constant(test_data[0:2],dtype=tf.float32),tf.constant(alloc1))
# print(kb.eval(envyvalue))
# # kb.eval()

# print('Validation Data')
# alloc1 = kb.eval(trained_model(test_data))
# calculateEnvySamplewise(tf.constant(test_data,dtype=tf.float32),tf.constant(alloc1))
# swcurrent = calculatesw(test_data,alloc1)
# print('Sw is ', kb.eval(swcurrent))

print()
print('Test Data')
sw_list = []
alpha_list = []

testsamples = 3

n_samples = 20000
for i in range(testsamples):
  test_data = gen_data(n_samples, n, m)
  alloc1 = kb.eval(trained_model(test_data))
  alpha = calculateEnvySamplewise(tf.constant(test_data,dtype=tf.float32),tf.constant(alloc1))
  swcurrent = calculatesw(test_data,alloc1)
  print('Sw is ', kb.eval(swcurrent))
  alpha_list.append(kb.eval(alpha))
  sw_list.append(kb.eval(swcurrent))

print(alpha_list)
print('For n_batches', testsamples , 'a_EF1  ', sum(alpha_list)/testsamples)
print('For n_batches', testsamples , 'b_EF1  ', sum(sw_list)/testsamples)

